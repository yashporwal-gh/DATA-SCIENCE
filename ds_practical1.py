# -*- coding: utf-8 -*-
"""DS_Practical1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WURpwYrjn6L4e40UWDSeGBVwXqBzzmhV
"""

pip install selenium

pip install beautifulsoup4

from selenium import webdriver
from bs4 import BeautifulSoup
import pandas as pd

!apt install chromium-chromedriver

from selenium import webdriver
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)
driver =webdriver.Chrome('chromedriver',chrome_options=chrome_options)

projects=[] #List to store name of the project
details=[] #List to store details of the project

driver.get("https://github.com/search?q=web+scraping")

content = driver.page_source
soup = BeautifulSoup(content, "html.parser")

names = soup.findAll('div', attrs={'class':'f4 text-normal'});
detail= soup.findAll('p',attrs={'class':'mb-1'})

for name in names:
  projects.append(name.text.strip())
  print(projects)
for de in detail:
  details.append(de.text)

print ("Number of items in the list = ", len(details))

data = {'projects': [projects], 'details': [details]}

df = pd.DataFrame(data=data)

print(df.head())

df.to_csv('projects.csv', index=False, encoding='utf-8')

#hactober-accepted
